"""
LLM Agent ëª¨ë“ˆ - Gemini ê¸°ë°˜ ReAct íŒ¨í„´ êµ¬í˜„.

Why: ìì—°ì–´ â†’ êµ¬ì¡°í™”ëœ ë°ì´í„° ë³€í™˜ì€ 100% LLMì´ ë‹´ë‹¹í•œë‹¤.
Toolì€ ISO í˜•ì‹ì˜ êµ¬ì¡°í™”ëœ ë°ì´í„°ë§Œ ì²˜ë¦¬í•œë‹¤. (CLAUDE.md ìˆœìˆ˜ LLM ì›ì¹™)
"""

import logging
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Optional
from collections import deque

import google.generativeai as genai
from google.generativeai.types import FunctionDeclaration, Tool, content_types

from config import config
from database import Database
from tools import TOOL_DEFINITIONS, execute_tool

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


# ============================================================
# ëŒ€í™” ë©”ëª¨ë¦¬ (Conversation Memory)
# ============================================================

@dataclass
class Message:
    """ëŒ€í™” ë©”ì‹œì§€ ë‹¨ìœ„."""
    role: str  # "user", "model", "function"
    content: str
    timestamp: datetime = field(default_factory=datetime.now)
    function_call: Optional[dict] = None
    function_response: Optional[dict] = None


class ConversationMemory:
    """
    ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤.

    Why: LLMì´ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ì¡°í•˜ì—¬ ë” ì •í™•í•œ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ í•œë‹¤.
    ìµœê·¼ Ní„´ë§Œ ìœ ì§€í•˜ì—¬ í† í° ì‚¬ìš©ëŸ‰ì„ ì œí•œí•œë‹¤.
    """

    def __init__(self, max_size: int = 10):
        """
        Args:
            max_size: ìœ ì§€í•  ìµœëŒ€ ëŒ€í™” í„´ ìˆ˜
        """
        self._messages: deque[Message] = deque(maxlen=max_size)
        self._max_size = max_size

    def add(self, role: str, content: str, **kwargs) -> None:
        """
        ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•œë‹¤.

        Args:
            role: ì—­í•  ("user", "model", "function")
            content: ë©”ì‹œì§€ ë‚´ìš©
            **kwargs: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (function_call, function_response ë“±)
        """
        msg = Message(role=role, content=content, **kwargs)
        self._messages.append(msg)
        logger.debug(f"Memory add: [{role}] {content[:50]}...")

    def get_context(self) -> list[dict]:
        """
        Gemini API í˜•ì‹ì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë°˜í™˜í•œë‹¤.

        Returns:
            [{"role": "user", "parts": ["..."]}] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
        """
        context = []
        for msg in self._messages:
            context.append({
                "role": msg.role,
                "parts": [msg.content],
            })
        return context

    def get_messages(self) -> list[Message]:
        """ëª¨ë“  ë©”ì‹œì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•œë‹¤."""
        return list(self._messages)

    def clear(self) -> None:
        """ëª¨ë“  ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì‚­ì œí•œë‹¤."""
        self._messages.clear()
        logger.debug("Memory cleared")

    def __len__(self) -> int:
        return len(self._messages)


# ============================================================
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ
# ============================================================

# Why: í”„ë¡¬í”„íŠ¸ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì½”ë“œ ë³€ê²½ ì—†ì´ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ê°€ëŠ¥
PROMPT_FILE_PATH = Path(__file__).parent / "prompt.md"


# ============================================================
# Gemini Tool ìŠ¤í‚¤ë§ˆ ë³€í™˜
# ============================================================

def build_gemini_tools() -> list[Tool]:
    """
    TOOL_DEFINITIONSë¥¼ Gemini Function Calling í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œë‹¤.

    Why: tools.pyì˜ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ Gemini APIê°€ ì´í•´í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜.
    """
    function_declarations = []

    for name, definition in TOOL_DEFINITIONS.items():
        params_schema = definition.get("parameters", {})

        # propertiesì™€ required ì¶”ì¶œ
        properties_raw = params_schema.get("properties", {})
        required = params_schema.get("required", [])

        # Gemini í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        properties = {}
        for param_name, param_info in properties_raw.items():
            prop = {
                "type": param_info["type"].upper(),
                "description": param_info["description"],
            }

            # enum ì²˜ë¦¬
            if "enum" in param_info:
                prop["enum"] = param_info["enum"]

            properties[param_name] = prop

        func_decl = FunctionDeclaration(
            name=name,
            description=definition["description"],
            parameters={
                "type": "OBJECT",
                "properties": properties,
                "required": required,
            } if properties else None,
        )
        function_declarations.append(func_decl)

    return [Tool(function_declarations=function_declarations)]


# ============================================================
# Agent í´ë˜ìŠ¤
# ============================================================

class Agent:
    """
    ReAct íŒ¨í„´ ê¸°ë°˜ LLM Agent.

    Why: ì‚¬ìš©ìì˜ ìì—°ì–´ ì…ë ¥ì„ ì´í•´í•˜ê³ , í•„ìš”í•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬
    ê²°ê³¼ë¥¼ ìì—°ì–´ ì‘ë‹µìœ¼ë¡œ ë³€í™˜í•œë‹¤.
    """

    def __init__(
        self,
        memory: Optional[ConversationMemory] = None,
        db: Optional[Database] = None,
    ):
        """
        Args:
            memory: ëŒ€í™” ë©”ëª¨ë¦¬ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
            db: ë°ì´í„°ë² ì´ìŠ¤ (ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œë¡œ ìƒì„±)
        """
        cfg = config()

        # Gemini API ì„¤ì •
        genai.configure(api_key=cfg.gemini_api_key)

        # ëª¨ë¸ ì´ˆê¸°í™”
        self._model = genai.GenerativeModel(
            model_name=cfg.gemini_flash_model,
            tools=build_gemini_tools(),
            system_instruction=self._build_system_prompt(),
        )

        # ëŒ€í™” ë©”ëª¨ë¦¬ (None ì²´í¬ - ë¹ˆ ë©”ëª¨ë¦¬ë„ ìœ íš¨í•¨)
        self._memory = memory if memory is not None else ConversationMemory(cfg.conversation_memory_size)

        # ë°ì´í„°ë² ì´ìŠ¤
        if db is not None:
            self._db = db
        else:
            self._db = Database(cfg.database_path)
            self._db.init_schema()

        # ReAct ì„¤ì •
        self._max_iterations = cfg.max_react_iterations

        logger.info(f"Agent initialized with model: {cfg.gemini_flash_model}")

    def _build_system_prompt(self) -> str:
        """
        í˜„ì¬ ë‚ ì§œ/ì‹œê°„ì´ í¬í•¨ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•œë‹¤.

        Why: prompt.md íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì½ì–´ì™€ì„œ ë™ì ìœ¼ë¡œ ë‚ ì§œ/ì‹œê°„ì„ ì‚½ì….
        íŒŒì¼ ë¶„ë¦¬ë¡œ ì½”ë“œ ë³€ê²½ ì—†ì´ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ì´ ê°€ëŠ¥í•´ì§.
        """
        now = datetime.now()

        try:
            prompt_template = PROMPT_FILE_PATH.read_text(encoding="utf-8")
            logger.debug(f"Loaded prompt from: {PROMPT_FILE_PATH}")
        except FileNotFoundError:
            logger.error(f"Prompt file not found: {PROMPT_FILE_PATH}")
            raise RuntimeError(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PROMPT_FILE_PATH}")
        except Exception as e:
            logger.error(f"Failed to read prompt file: {e}")
            raise RuntimeError(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")

        return prompt_template.format(
            today=now.strftime("%Y-%m-%d (%A)"),
            now=now.strftime("%H:%M"),
        )

    async def process_message(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ë°˜í™˜í•œë‹¤.

        Why: ReAct íŒ¨í„´ìœ¼ë¡œ ë„êµ¬ í˜¸ì¶œ â†’ ê²°ê³¼ í™•ì¸ â†’ ì¶”ê°€ í˜¸ì¶œ/ì‘ë‹µ ìƒì„±ì„ ë°˜ë³µ.

        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€

        Returns:
            AI ì‘ë‹µ ë©”ì‹œì§€
        """
        logger.info(f"Processing: {user_input[:50]}...")

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        self._memory.add("user", user_input)

        # ëŒ€í™” ì‹œì‘
        chat = self._model.start_chat(history=self._memory.get_context()[:-1])

        # ReAct ë£¨í”„
        iteration = 0
        response = None

        while iteration < self._max_iterations:
            iteration += 1
            logger.debug(f"ReAct iteration {iteration}")

            # LLM í˜¸ì¶œ
            if response is None:
                # ì²« í˜¸ì¶œ
                response = await chat.send_message_async(user_input)
            else:
                # ë„êµ¬ ê²°ê³¼ í›„ í›„ì† í˜¸ì¶œ
                response = await chat.send_message_async(tool_response_parts)

            # ì‘ë‹µ ë¶„ì„
            candidate = response.candidates[0]
            content = candidate.content

            # Function Call í™•ì¸
            function_calls = []
            text_parts = []

            for part in content.parts:
                if hasattr(part, "function_call") and part.function_call:
                    function_calls.append(part.function_call)
                elif hasattr(part, "text") and part.text:
                    text_parts.append(part.text)

            # Function Callì´ ì—†ìœ¼ë©´ ìµœì¢… ì‘ë‹µ
            if not function_calls:
                final_response = "".join(text_parts)
                self._memory.add("model", final_response)
                logger.info(f"Final response: {final_response[:50]}...")
                return final_response

            # Function Call ì‹¤í–‰
            tool_response_parts = []

            for fc in function_calls:
                tool_name = fc.name
                tool_args = dict(fc.args) if fc.args else {}

                logger.info(f"Tool call: {tool_name}({tool_args})")

                # ë„êµ¬ ì‹¤í–‰
                try:
                    result = execute_tool(self._db, tool_name, tool_args)
                except Exception as e:
                    logger.error(f"Tool error: {e}")
                    result = {"success": False, "error": str(e)}

                logger.info(f"Tool result: {result}")

                # Geminiì— ì „ë‹¬í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                tool_response_parts.append(
                    content_types.to_part({
                        "function_response": {
                            "name": tool_name,
                            "response": result,
                        }
                    })
                )

        # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì´ˆê³¼
        logger.warning(f"Max iterations ({self._max_iterations}) exceeded")
        return "ì£„ì†¡í•´ìš”, ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ˜…"

    def clear_memory(self) -> None:
        """ëŒ€í™” ë©”ëª¨ë¦¬ë¥¼ ì´ˆê¸°í™”í•œë‹¤."""
        self._memory.clear()
        logger.info("Memory cleared")

    @property
    def memory(self) -> ConversationMemory:
        """ëŒ€í™” ë©”ëª¨ë¦¬ë¥¼ ë°˜í™˜í•œë‹¤."""
        return self._memory
