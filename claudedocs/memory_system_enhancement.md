# Angmini ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ê³ ë„í™” ì„¤ê³„ì„œ

> **ì‘ì„±ì¼**: 2025-10-03
> **ëª©ì **: ìµœì‹  AI ì—ì´ì „íŠ¸ ë©”ëª¨ë¦¬ ì•Œê³ ë¦¬ì¦˜ ì¡°ì‚¬ ë° Angmini í”„ë¡œì íŠ¸ ì ìš©
> **ë²„ì „**: 1.0.0

---

## ğŸ“‹ ëª©ì°¨

1. [ì¡°ì‚¬ ë°°ê²½ ë° ëª©ì ](#1-ì¡°ì‚¬-ë°°ê²½-ë°-ëª©ì )
2. [ìµœì‹  AI ì—ì´ì „íŠ¸ ë©”ëª¨ë¦¬ ì•Œê³ ë¦¬ì¦˜ ì¡°ì‚¬](#2-ìµœì‹ -ai-ì—ì´ì „íŠ¸-ë©”ëª¨ë¦¬-ì•Œê³ ë¦¬ì¦˜-ì¡°ì‚¬)
3. [í˜„ì¬ Angmini ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ë¶„ì„](#3-í˜„ì¬-angmini-ë©”ëª¨ë¦¬-ì‹œìŠ¤í…œ-ë¶„ì„)
4. [ê°œì„  ë°©ì•ˆ ì„¤ê³„](#4-ê°œì„ -ë°©ì•ˆ-ì„¤ê³„)
5. [êµ¬í˜„ ë¡œë“œë§µ](#5-êµ¬í˜„-ë¡œë“œë§µ)
6. [ì˜ˆìƒ íš¨ê³¼](#6-ì˜ˆìƒ-íš¨ê³¼)

---

## 1. ì¡°ì‚¬ ë°°ê²½ ë° ëª©ì 

### 1.1 ë°°ê²½

AngminiëŠ” í˜„ì¬ ì¥ê¸° ê¸°ì–µ(Long-Term Memory) ì‹œìŠ¤í…œì„ ê°–ì¶”ê³  ìˆìœ¼ë‚˜, 2024-2025ë…„ì— ë°œí‘œëœ ìµœì‹  AI ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ë“¤ì€ ë” ì •êµí•œ ê³„ì¸µì  ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜ë¥¼ ë„ì…í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### 1.2 ëª©ì 

- **ìµœì‹  ì•Œê³ ë¦¬ì¦˜ íŒŒì•…**: MemGPT, AutoGen v0.4, Mem0, CrewAI ë“±ì˜ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¡°ì‚¬
- **í˜„ì¬ ì‹œìŠ¤í…œ í‰ê°€**: Angmini ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì˜ ê°•ì  ë° ê°œì„ ì  ë¶„ì„
- **ì‹¤ìš©ì  ê°œì„ **: ì¡°ì‚¬í•œ ì•Œê³ ë¦¬ì¦˜ì„ Angminiì— ì ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì„±ëŠ¥ í–¥ìƒ

---

## 2. ìµœì‹  AI ì—ì´ì „íŠ¸ ë©”ëª¨ë¦¬ ì•Œê³ ë¦¬ì¦˜ ì¡°ì‚¬

### 2.1 ê³„ì¸µì  ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜

#### MemGPT / Letta (2024-2025)

**í•µì‹¬ ê°œë…**: OS ìŠ¤íƒ€ì¼ ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡°

- **ë©”ëª¨ë¦¬ ê³„ì¸µ**:
  - Working Context (ê³ ì • ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ë‚´)
  - External Storage (ë¬´ì œí•œ ìš©ëŸ‰)
  - Active Memory Management (ì—ì´ì „íŠ¸ê°€ ëŠ¥ë™ì ìœ¼ë¡œ ê´€ë¦¬)

- **íŠ¹ì§•**:
  - ì—ì´ì „íŠ¸ê°€ ë¬´ì—‡ì„ ì»¨í…ìŠ¤íŠ¸ì— ìœ ì§€í•˜ê³  ë¬´ì—‡ì„ ì™¸ë¶€ë¡œ ë³´ë‚¼ì§€ ê²°ì •
  - í•„ìš”ì‹œ ì™¸ë¶€ ë ˆì´ì–´ì—ì„œ ê²€ìƒ‰í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¡œ ë¡œë“œ
  - **ë²¤ì¹˜ë§ˆí¬**: LoCoMo 74.0% (GPT-4o mini)

- **ì°¸ê³ **: https://www.letta.com/

#### CrewAI Memory System (2024-2025)

**í•µì‹¬ ê°œë…**: 4ê³„ì¸µ í†µí•© ë©”ëª¨ë¦¬

```python
Component | Description
----------|------------
Short-Term Memory | í˜„ì¬ ì‹¤í–‰ ì¤‘ RAG ê¸°ë°˜ ìµœê·¼ ìƒí˜¸ì‘ìš© ì €ì¥ (ChromaDB)
Long-Term Memory | ì„¸ì…˜ ê°„ ë³´ì¡´, ê³¼ê±° ì‹¤í–‰ì—ì„œ í•™ìŠµ (SQLite3)
Entity Memory | ì‚¬ëŒ/ì¥ì†Œ/ê°œë… ì¶”ì  ë° ê´€ê³„ ë§¤í•‘ (RAG)
Contextual Memory | ìœ„ 3ê°œ í†µí•©í•˜ì—¬ ì¼ê´€ëœ ì‘ë‹µ ì œê³µ
```

- **ì €ì¥ì†Œ**:
  - Short-Term: ChromaDB + RAG
  - Long-Term: SQLite3
  - Entity: RAG ê¸°ë°˜
  - Custom storage path ì§€ì›

- **ì„¤ì •**:
```python
crew = Crew(
    agents=[...],
    tasks=[...],
    memory=True,  # ëª¨ë“  ë©”ëª¨ë¦¬ í™œì„±í™”
    embedder={"provider": "huggingface", "config": {...}}
)
```

- **ì°¸ê³ **: https://docs.crewai.com/concepts/memory

#### AutoGen v0.4 (2025ë…„ 1ì›”)

**í•µì‹¬ ê°œë…**: ëª¨ë“ˆì‹ ë° í™•ì¥ ê°€ëŠ¥ ì•„í‚¤í…ì²˜

- **íŠ¹ì§•**:
  - í”ŒëŸ¬ê·¸ ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ (memory, model, tools)
  - Memory protocolë¡œ RAG íŒ¨í„´ ì§€ì›
  - AgentChatì— Memory ì¸í„°í˜ì´ìŠ¤ ì œê³µ

- **Memory Protocol**:
```python
class Memory(Protocol):
    def query(self, query: str, limit: int) -> List[Result]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ê´€ë ¨ ë©”ëª¨ë¦¬ ë°˜í™˜"""
        ...

    def save(self, value: str, metadata: Dict) -> None:
        """ìƒˆ ë©”ëª¨ë¦¬ ì €ì¥"""
        ...
```

- **ì°¸ê³ **: https://microsoft.github.io/autogen/

#### Mem0 Platform (2024-2025)

**í•µì‹¬ ê°œë…**: í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„°ë² ì´ìŠ¤ + ìŠ¤ë§ˆíŠ¸ ìê°€ ê°œì„  ë©”ëª¨ë¦¬

- **í•˜ì´ë¸Œë¦¬ë“œ DB**:
  - Vector DB: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê²€ìƒ‰
  - Key-Value Store: ë¹ ë¥¸ ì¡°íšŒ
  - Graph DB: ê´€ê³„ íƒìƒ‰

- **íŠ¹ì§•**:
  - ê³ ìœ  ì‹ë³„ì (user_id, org_id, project_id)
  - ì •êµí•œ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤
  - ì‚¬ìš©ì ë§ì¶¤í˜• ì¹´í…Œê³ ë¦¬

- **ì°¸ê³ **: Mem0 ë…¼ë¬¸ (arXiv:2504.19413)

### 2.2 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ

#### Reciprocal Rank Fusion (RRF)

**ê°œë…**: ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³‘í•©

**ì•Œê³ ë¦¬ì¦˜**:
```python
def rrf_score(rank, k=60):
    return 1 / (rank + k)

# ê° ê²°ê³¼ì˜ ë­í¬ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜ í›„ í•©ì‚°
for rank, doc in enumerate(semantic_results):
    scores[doc.id] += rrf_score(rank)

for rank, doc in enumerate(keyword_results):
    scores[doc.id] += rrf_score(rank)

# ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)
```

**ì¥ì **:
- ì˜ë¯¸ì  ìœ ì‚¬ë„ì™€ ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ë™ì‹œ í™œìš©
- ë‹¨ìˆœí•˜ë©´ì„œë„ íš¨ê³¼ì 
- ê°€ì¤‘ì¹˜ ì¡°ì • ê°€ëŠ¥ (alpha íŒŒë¼ë¯¸í„°)

#### ì‹œê°„ ê¸°ë°˜ ê°ì‡  (Temporal Decay)

**ê°œë…**: ìµœì‹  ë©”ëª¨ë¦¬ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬

**ê³µì‹**:
```python
score_adjusted = score * exp(-decay_rate * age_days)
```

**íŒŒë¼ë¯¸í„°**:
- `decay_rate`: 0.01 ~ 0.05 (ì¼ë°˜ì )
- `age_days`: ìƒì„± í›„ ê²½ê³¼ ì¼ìˆ˜

### 2.3 ì—”í‹°í‹° ì¶”ì  ë° ê´€ê³„ ë§¤í•‘

#### Graph-based Entity Memory

**ê°œë…**: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§

**ì—”í‹°í‹° íƒ€ì…**:
- PERSON: ì‚¬ìš©ì, íŒ€ì›
- PROJECT: í”„ë¡œì íŠ¸, ì‘ì—…
- TOOL: ì‚¬ìš©í•œ ë„êµ¬, API
- CONCEPT: ê°œë…, ê¸°ìˆ  ìŠ¤íƒ
- FILE: íŒŒì¼, ë””ë ‰í† ë¦¬

**ê´€ê³„ íƒ€ì…**:
- WORKS_ON: Person â†’ Project
- USES: Person â†’ Tool
- CONTAINS: Project â†’ File
- RELATES_TO: Concept â†’ Concept

**ì¥ì **:
- ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í–¥ìƒ
- "ì´ ì‚¬ìš©ìê°€ ì´ì „ì— ì‘ì—…í•œ í”„ë¡œì íŠ¸ëŠ”?" ê°™ì€ ì¿¼ë¦¬ ì§€ì›
- ê°œì¸í™”ëœ ì‘ë‹µ ìƒì„±

### 2.4 ë©”ëª¨ë¦¬ í†µí•© ë° ì••ì¶•

#### Memory Consolidation (MemoryOS ë°©ì‹)

**ê°œë…**: ìœ ì‚¬í•œ ë©”ëª¨ë¦¬ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ìŠ¤í† ë¦¬ì§€ ìµœì í™”

**í”„ë¡œì„¸ìŠ¤**:
1. **í´ëŸ¬ìŠ¤í„°ë§**: ìœ ì‚¬í•œ ë©”ëª¨ë¦¬ ê·¸ë£¹í™” (ì„ë² ë”© ê±°ë¦¬ ê¸°ë°˜)
2. **ìš”ì•½**: LLMìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ìš”ì•½
3. **ì•„ì¹´ì´ë¹™**: ì›ë³¸ ë©”ëª¨ë¦¬ì— `archived` í”Œë˜ê·¸ ì„¤ì •
4. **í†µí•© ë ˆì½”ë“œ ìƒì„±**: ìš”ì•½ë³¸ì„ ìƒˆ ë©”ëª¨ë¦¬ë¡œ ì €ì¥

**íŠ¸ë¦¬ê±°**:
- ë©”ëª¨ë¦¬ ìˆ˜ > 1000ê°œ
- ì •ê¸° ìŠ¤ì¼€ì¤„ (ì˜ˆ: ë§¤ì¼ ìì •)

#### Importance Scoring

**ê°œë…**: ë©”ëª¨ë¦¬ì˜ ì¤‘ìš”ë„ë¥¼ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ê´€ë¦¬

**ìš”ì¸**:
```python
importance = (
    0.3 * access_frequency +    # ì ‘ê·¼ ë¹ˆë„
    0.3 * recency_score +       # ìµœê·¼ì„±
    0.2 * success_score +       # ì„±ê³µ ì—¬ë¶€
    0.2 * user_feedback_score  # ì‚¬ìš©ì í”¼ë“œë°±
)
```

**í™œìš©**:
- `importance < 0.2 and age > 90 days` â†’ ìë™ ì•„ì¹´ì´ë¹™
- ê²€ìƒ‰ ì‹œ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ì ìš©

---

## 3. í˜„ì¬ Angmini ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ë¶„ì„

### 3.1 í˜„ì¬ ì•„í‚¤í…ì²˜

```
Angmini Memory System (í˜„ì¬)
â”œâ”€ Storage
â”‚  â”œâ”€ SQLite (ë©”íƒ€ë°ì´í„°)
â”‚  â”œâ”€ FAISS (ë²¡í„° ì¸ë±ìŠ¤)
â”‚  â””â”€ Qwen3-Embedding-0.6B (ì„ë² ë”©)
â”‚
â”œâ”€ Retrieval
â”‚  â”œâ”€ CascadedRetriever (ë‹¤ì¤‘ í™‰ LLM í•„í„°ë§)
â”‚  â””â”€ Repository.search() (ë‹¨ìˆœ ë²¡í„° ê²€ìƒ‰)
â”‚
â”œâ”€ Pipeline
â”‚  â”œâ”€ SnapshotExtractor (ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ)
â”‚  â”œâ”€ MemoryCurator (LLM ìš”ì•½)
â”‚  â”œâ”€ MemoryDeduplicator (ì¤‘ë³µ ì œê±°)
â”‚  â””â”€ MemoryRetentionPolicy (ì €ì¥ ê²°ì •)
â”‚
â””â”€ Categories
   â”œâ”€ FULL_EXPERIENCE
   â”œâ”€ ERROR_SOLUTION
   â”œâ”€ TOOL_USAGE
   â”œâ”€ USER_PATTERN
   â””â”€ WORKFLOW_OPTIMISATION
```

### 3.2 MemoryRecord êµ¬ì¡°

```python
@dataclass
class MemoryRecord:
    summary: str                    # LLM ìƒì„± ìš”ì•½
    goal: str                       # ì‚¬ìš©ì ëª©í‘œ
    user_intent: str                # ì˜ë„ ë¶„ì„
    outcome: str                    # ê²°ê³¼
    category: MemoryCategory        # ì¹´í…Œê³ ë¦¬
    tools_used: List[str]           # ì‚¬ìš© ë„êµ¬
    tags: List[str]                 # íƒœê·¸
    created_at: datetime            # ìƒì„± ì‹œê°
    source_metadata: Dict[str, Any] # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    embedding: Optional[List[float]] # ë²¡í„° ì„ë² ë”©
```

### 3.3 ê°•ì 

âœ… **ê³ ê¸‰ Cascaded Retrieval**:
- ë‹¤ì¤‘ í™‰ ê²€ìƒ‰
- LLM ê¸°ë°˜ ê´€ë ¨ì„± í•„í„°ë§
- ë°˜ë³µì  ì¿¼ë¦¬ í™•ì¥
- í´ë°± ë©”ì»¤ë‹ˆì¦˜

âœ… **LLM ê¸°ë°˜ íë ˆì´ì…˜**:
- MemoryCuratorê°€ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ìš”ì•½ìœ¼ë¡œ ë³€í™˜
- ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
- íƒœê·¸ ìë™ ìƒì„±

âœ… **ì˜ë¯¸ì  ê²€ìƒ‰**:
- Qwen3 ì„ë² ë”© + FAISS
- ê³ í’ˆì§ˆ ë²¡í„° ê²€ìƒ‰

âœ… **ì¤‘ë³µ ì œê±°**:
- MemoryDeduplicatorë¡œ ìœ ì‚¬ ë©”ëª¨ë¦¬ ê°ì§€

### 3.4 ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„

âŒ **ë‹¨ì¼ ê³„ì¸µ ë©”ëª¨ë¦¬**:
- Long-Term Memoryë§Œ ì¡´ì¬
- Short-Term Memory ì—†ìŒ (í˜„ì¬ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ë¶€ì¬)

âŒ **ì—”í‹°í‹° ì¶”ì  ë¶€ì¬**:
- ì‚¬ëŒ, í”„ë¡œì íŠ¸, ë„êµ¬, ê°œë… ê°„ ê´€ê³„ ë¯¸ì¶”ì 
- "ì´ ì‚¬ìš©ìê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ëŠ”?" ê°™ì€ ì¿¼ë¦¬ ë¶ˆê°€

âŒ **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¶€ì¬**:
- ì˜ë¯¸ì  ê²€ìƒ‰ë§Œ ì§€ì›
- ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ë¶ˆê°€ (ì˜ˆ: íŠ¹ì • íŒŒì¼ëª… ê²€ìƒ‰)

âŒ **ì‹œê°„ ê¸°ë°˜ ê´€ë¦¬ ë¶€ì¡±**:
- ìµœê·¼ì„± ê°€ì¤‘ì¹˜ ì—†ìŒ
- ì¤‘ìš”ë„ ì ìˆ˜ ì‹œìŠ¤í…œ ì—†ìŒ
- ìë™ ì•„ì¹´ì´ë¹™ ì—†ìŒ

âŒ **ë©”ëª¨ë¦¬ í†µí•©/ì••ì¶• ì—†ìŒ**:
- ë©”ëª¨ë¦¬ ìˆ˜ ì¦ê°€ ì‹œ ì„±ëŠ¥ ì €í•˜ ìš°ë ¤
- ìœ ì‚¬í•œ ë©”ëª¨ë¦¬ ìˆ˜ë™ ì •ë¦¬ í•„ìš”

---

## 4. ê°œì„  ë°©ì•ˆ ì„¤ê³„

### 4.1 Phase 1: ê³„ì¸µì  ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜

#### 4.1.1 Short-Term Memory (CrewAI í†µí•©)

**ëª©ì **: í˜„ì¬ ì„¸ì…˜/ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€

**êµ¬í˜„ ì „ëµ**:
- CrewAIì˜ built-in Short-Term Memory í™œìš©
- ChromaDB ê¸°ë°˜ (CrewAI ê¸°ë³¸ ì œê³µ)

**ì„¤ì •**:
```python
# crew/crew_config.py
crew = Crew(
    agents=[...],
    tasks=[...],
    memory=True,  # Short-Term + Long-Term + Entity í™œì„±í™”
    embedder={
        "provider": "huggingface",
        "config": {"model": "Qwen/Qwen3-Embedding-0.6B"}
    }
)
```

**ìƒëª…ì£¼ê¸°**:
- ì„¸ì…˜ ì‹œì‘: ë¹ˆ ìƒíƒœ
- ì„¸ì…˜ ì¤‘: ìµœê·¼ Nê°œ ìƒí˜¸ì‘ìš© ì €ì¥ (N=20)
- ì„¸ì…˜ ì¢…ë£Œ: ì¤‘ìš”í•œ ë‚´ìš©ë§Œ Long-Termìœ¼ë¡œ ì „í™˜

#### 4.1.2 Entity Memory

**ëª©ì **: ì‚¬ëŒ, í”„ë¡œì íŠ¸, ë„êµ¬, ê°œë… ì¶”ì  ë° ê´€ê³„ ë§¤í•‘

**ë””ë ‰í† ë¦¬ êµ¬ì¡°**:
```
ai/memory/entity/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ models.py           # Entity, EntityType, EntityRelation
â”œâ”€â”€ extractor.py        # LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ
â”œâ”€â”€ storage.py          # EntityRepository (SQLite + FAISS)
â””â”€â”€ tracker.py          # EntityTracker (ì „ì²´ ê´€ë¦¬)
```

**ë°ì´í„° ëª¨ë¸**:
```python
class EntityType(Enum):
    PERSON = "person"
    PROJECT = "project"
    TOOL = "tool"
    CONCEPT = "concept"
    FILE = "file"

@dataclass
class Entity:
    id: str                              # UUID
    type: EntityType                     # ì—”í‹°í‹° íƒ€ì…
    name: str                            # ì´ë¦„
    attributes: Dict[str, Any]           # ì†ì„± (flexible)
    first_seen: datetime                 # ì²˜ìŒ ë°œê²¬
    last_seen: datetime                  # ë§ˆì§€ë§‰ ì–¸ê¸‰
    mention_count: int                   # ì–¸ê¸‰ íšŸìˆ˜
    relations: List[Tuple[str, str, str]] # (relation_type, target_id, context)
    embedding: Optional[List[float]]     # ë²¡í„° ì„ë² ë”©
```

**EntityExtractor**:
```python
class EntityExtractor:
    def __init__(self, brain: AIBrain):
        self.brain = brain
        self.prompt_template = self._load_prompt()

    def extract(self, text: str) -> List[Entity]:
        """LLMìœ¼ë¡œ Named Entity Recognition ìˆ˜í–‰"""
        prompt = self.prompt_template.replace("{{text}}", text)
        response = self.brain.generate_text(prompt, temperature=0.1)

        # JSON íŒŒì‹± ë° Entity ê°ì²´ ìƒì„±
        entities_data = json.loads(response.text)
        return [Entity(**e) for e in entities_data["entities"]]
```

**EntityRepository**:
```python
class EntityRepository:
    def __init__(self, sqlite_store, vector_index):
        self.sqlite = sqlite_store
        self.vector_index = vector_index

    def add_or_update(self, entity: Entity) -> Entity:
        """ê¸°ì¡´ ì—”í‹°í‹° ì—…ë°ì´íŠ¸ or ìƒˆë¡œ ìƒì„±"""
        existing = self.find_by_name(entity.name, entity.type)
        if existing:
            existing.mention_count += 1
            existing.last_seen = datetime.utcnow()
            existing.relations.extend(entity.relations)
            self.sqlite.update(existing)
            return existing
        else:
            self.sqlite.insert(entity)
            self.vector_index.add(entity)
            return entity

    def find_by_name(self, name: str, type: EntityType = None) -> Optional[Entity]:
        """ì´ë¦„ìœ¼ë¡œ ì—”í‹°í‹° ê²€ìƒ‰"""
        pass

    def find_related(self, entity_id: str, relation_type: str = None) -> List[Entity]:
        """ê´€ê³„ëœ ì—”í‹°í‹° ê²€ìƒ‰"""
        pass

    def search_semantic(self, query: str, top_k: int = 5) -> List[Tuple[Entity, float]]:
        """ì˜ë¯¸ì  ê²€ìƒ‰"""
        pass
```

**í†µí•© ì˜ˆì‹œ**:
```python
# ë©”ëª¨ë¦¬ ìº¡ì²˜ ì‹œ ì—”í‹°í‹° ìë™ ì¶”ì¶œ
def capture_with_entities(context, user_request):
    # 1. ê¸°ì¡´ ë©”ëª¨ë¦¬ ìº¡ì²˜
    memory_result = memory_service.capture(context, user_request)

    # 2. ì—”í‹°í‹° ì¶”ì¶œ
    entities = entity_extractor.extract(context.to_text())

    # 3. ì—”í‹°í‹° ì €ì¥ ë° ì—…ë°ì´íŠ¸
    for entity in entities:
        entity_repository.add_or_update(entity)

    return memory_result
```

#### 4.1.3 Contextual Memory (í†µí•© ë ˆì´ì–´)

**ëª©ì **: Short-Term, Long-Term, Entityë¥¼ í†µí•©í•œ ì¼ê´€ëœ ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤

**êµ¬í˜„**:
```python
class ContextualMemoryService:
    def __init__(self,
                 short_term_memory,      # CrewAI Memory
                 long_term_memory,        # Angmini MemoryRepository
                 entity_memory):          # EntityRepository
        self.stm = short_term_memory
        self.ltm = long_term_memory
        self.entity = entity_memory

    def retrieve(self, query: str, top_k: int = 10, context: str = "all") -> List[Any]:
        """í†µí•© ê²€ìƒ‰"""
        results = []

        # 1. Short-Term (ìµœê·¼ ì»¨í…ìŠ¤íŠ¸)
        if context in ["all", "short"]:
            stm_results = self.stm.search(query, limit=5)
            results.extend([("short", r) for r in stm_results])

        # 2. Entity (ê´€ë ¨ ì—”í‹°í‹°)
        if context in ["all", "entity"]:
            entity_results = self.entity.search_semantic(query, top_k=5)
            results.extend([("entity", e) for e, score in entity_results])

        # 3. Long-Term (ê³¼ê±° ê²½í—˜)
        if context in ["all", "long"]:
            ltm_results = self.ltm.cascaded_retriever.retrieve(query)
            results.extend([("long", m) for m in ltm_results.matches])

        # 4. ê²°ê³¼ ë³‘í•© ë° ë¦¬ë­í‚¹
        return self._merge_and_rank(results, top_k=top_k)

    def _merge_and_rank(self, results, top_k):
        """ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ê²°ê³¼ë¥¼ í†µí•© ì ìˆ˜ë¡œ ë¦¬ë­í‚¹"""
        # ê° ì†ŒìŠ¤ì— ê°€ì¤‘ì¹˜ ì ìš©
        weights = {"short": 0.4, "entity": 0.3, "long": 0.3}

        # ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        scored = []
        for source, item in results:
            base_score = self._get_score(item)
            final_score = base_score * weights[source]
            scored.append((item, final_score))

        scored.sort(key=lambda x: x[1], reverse=True)
        return [item for item, score in scored[:top_k]]
```

### 4.2 Phase 2: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ

#### 4.2.1 FTS5 Full-Text Search

**ëª©ì **: ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì§€ì›

**SQLite FTS5 í…Œì´ë¸” ìƒì„±**:
```sql
CREATE VIRTUAL TABLE IF NOT EXISTS memories_fts
USING fts5(
    id UNINDEXED,
    summary,
    goal,
    user_intent,
    outcome,
    tags,
    tokenize='unicode61'
);

-- íŠ¸ë¦¬ê±°: ë©”ëª¨ë¦¬ ì¶”ê°€ ì‹œ FTS ìë™ ì—…ë°ì´íŠ¸
CREATE TRIGGER IF NOT EXISTS memories_fts_insert
AFTER INSERT ON memories
BEGIN
    INSERT INTO memories_fts(id, summary, goal, user_intent, outcome, tags)
    VALUES (NEW.id, NEW.summary, NEW.goal, NEW.user_intent, NEW.outcome, NEW.tags);
END;
```

**FTS ê²€ìƒ‰ êµ¬í˜„**:
```python
def fts_search(self, query: str, top_k: int = 10) -> List[Tuple[MemoryRecord, float]]:
    """SQLite FTS5ë¡œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
    # FTS5 ì¿¼ë¦¬ (BM25 ë­í‚¹)
    sql = """
        SELECT id, rank
        FROM memories_fts
        WHERE memories_fts MATCH ?
        ORDER BY rank
        LIMIT ?
    """

    cursor = self.sqlite.execute(sql, (query, top_k))
    results = []

    for row in cursor:
        record_id, fts_rank = row
        record = self.get_by_id(record_id)
        # FTS5 rankëŠ” ìŒìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ) â†’ ì–‘ìˆ˜ ì ìˆ˜ë¡œ ë³€í™˜
        score = 1.0 / (1.0 - fts_rank)
        results.append((record, score))

    return results
```

#### 4.2.2 Reciprocal Rank Fusion (RRF)

**êµ¬í˜„**:
```python
class HybridRetriever:
    def __init__(self, vector_index, sqlite_store, alpha=0.5, k=60):
        self.vector_index = vector_index
        self.sqlite_store = sqlite_store
        self.alpha = alpha  # ì˜ë¯¸ì  vs í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        self.k = k          # RRF ìƒìˆ˜

    def search(self, query: str, top_k: int = 10) -> List[Tuple[MemoryRecord, float]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ì˜ë¯¸ì  + í‚¤ì›Œë“œ"""

        # 1. ì˜ë¯¸ì  ê²€ìƒ‰
        semantic_results = self.vector_index.search(query, k=top_k * 2)

        # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ (FTS5)
        keyword_results = self.sqlite_store.fts_search(query, top_k=top_k * 2)

        # 3. RRF ë³‘í•©
        return self._reciprocal_rank_fusion(
            semantic_results, keyword_results,
            alpha=self.alpha, k_param=self.k, top_k=top_k
        )

    def _reciprocal_rank_fusion(self, semantic, keyword, alpha, k_param, top_k):
        """RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²°ê³¼ ë³‘í•©"""
        scores = {}

        # ì˜ë¯¸ì  ê²€ìƒ‰ ì ìˆ˜
        for rank, (record, _) in enumerate(semantic):
            record_id = record.source_metadata["id"]
            rrf_score = alpha / (rank + k_param)
            scores[record_id] = scores.get(record_id, 0) + rrf_score

        # í‚¤ì›Œë“œ ê²€ìƒ‰ ì ìˆ˜
        for rank, (record, _) in enumerate(keyword):
            record_id = record.source_metadata["id"]
            rrf_score = (1 - alpha) / (rank + k_param)
            scores[record_id] = scores.get(record_id, 0) + rrf_score

        # ì •ë ¬ ë° ë°˜í™˜
        sorted_ids = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]

        results = []
        for record_id, score in sorted_ids:
            record = self.get_by_id(record_id)
            results.append((record, score))

        return results
```

#### 4.2.3 ì‹œê°„ ê¸°ë°˜ ê°ì‡ 

**êµ¬í˜„**:
```python
def apply_temporal_decay(self, score: float, created_at: datetime,
                        decay_rate: float = 0.01) -> float:
    """ì‹œê°„ ê¸°ë°˜ ì ìˆ˜ ê°ì‡ """
    age_days = (datetime.utcnow() - created_at).days
    decay_factor = math.exp(-decay_rate * age_days)
    return score * decay_factor
```

**í†µí•©**:
```python
def search_with_decay(self, query: str, top_k: int = 10, decay_rate: float = 0.01):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ì‹œê°„ ê°ì‡ """
    raw_results = self.hybrid_search(query, top_k=top_k * 2)

    # ì‹œê°„ ê°ì‡  ì ìš©
    decayed = []
    for record, score in raw_results:
        adjusted_score = self.apply_temporal_decay(
            score, record.created_at, decay_rate
        )
        decayed.append((record, adjusted_score))

    # ì¬ì •ë ¬ ë° ë°˜í™˜
    decayed.sort(key=lambda x: x[1], reverse=True)
    return decayed[:top_k]
```

### 4.3 Phase 3: ì¤‘ìš”ë„ ì ìˆ˜ ì‹œìŠ¤í…œ

#### 4.3.1 ImportanceCalculator

**íŒŒì¼**: `ai/memory/importance.py`

```python
@dataclass
class ImportanceFactors:
    access_count: int = 0                      # ì ‘ê·¼ íšŸìˆ˜
    last_accessed: Optional[datetime] = None   # ë§ˆì§€ë§‰ ì ‘ê·¼
    created_at: datetime = field(default_factory=datetime.utcnow)
    success_indicator: float = 0.5             # 0.0 ~ 1.0
    user_feedback: Optional[float] = None      # explicit rating

@dataclass
class ImportanceScore:
    total: float                               # 0.0 ~ 1.0
    breakdown: Dict[str, float]                # ìš”ì¸ë³„ ì ìˆ˜

class ImportanceCalculator:
    def __init__(self,
                 access_weight: float = 0.3,
                 recency_weight: float = 0.3,
                 success_weight: float = 0.2,
                 feedback_weight: float = 0.2,
                 decay_rate: float = 0.01):
        self.weights = {
            'access': access_weight,
            'recency': recency_weight,
            'success': success_weight,
            'feedback': feedback_weight
        }
        self.decay_rate = decay_rate

    def calculate(self, factors: ImportanceFactors) -> ImportanceScore:
        # 1. Access frequency (ë¡œê·¸ ì •ê·œí™”)
        access_score = min(1.0, math.log(factors.access_count + 1) / math.log(100))

        # 2. Recency (ì§€ìˆ˜ ê°ì‡ )
        age_days = (datetime.utcnow() - factors.created_at).days
        recency_score = math.exp(-self.decay_rate * age_days)

        # 3. Success indicator
        success_score = factors.success_indicator

        # 4. User feedback (ê¸°ë³¸ 0.5)
        feedback_score = factors.user_feedback if factors.user_feedback is not None else 0.5

        # ê°€ì¤‘ í•©
        total = (
            self.weights['access'] * access_score +
            self.weights['recency'] * recency_score +
            self.weights['success'] * success_score +
            self.weights['feedback'] * feedback_score
        )

        return ImportanceScore(
            total=total,
            breakdown={
                'access': access_score,
                'recency': recency_score,
                'success': success_score,
                'feedback': feedback_score
            }
        )
```

#### 4.3.2 MemoryRecord í™•ì¥

```python
@dataclass
class MemoryRecord:
    # ê¸°ì¡´ í•„ë“œë“¤
    summary: str
    goal: str
    user_intent: str
    outcome: str
    category: MemoryCategory
    tools_used: List[str]
    tags: List[str]
    created_at: datetime
    source_metadata: Dict[str, Any]
    embedding: Optional[List[float]]

    # ìƒˆë¡œ ì¶”ê°€
    access_count: int = 0
    last_accessed: Optional[datetime] = None
    importance_score: Optional[float] = None
    archived: bool = False
```

#### 4.3.3 ìë™ ì•„ì¹´ì´ë¹™

```python
class MemoryArchiver:
    def __init__(self, repository, importance_calculator,
                 importance_threshold=0.2, age_threshold_days=90):
        self.repository = repository
        self.calculator = importance_calculator
        self.importance_threshold = importance_threshold
        self.age_threshold = age_threshold_days

    def run_archival_pass(self):
        """ì¤‘ìš”ë„ ë‚®ê³  ì˜¤ë˜ëœ ë©”ëª¨ë¦¬ ì•„ì¹´ì´ë¹™"""
        all_memories = self.repository.list_all()
        archived_count = 0

        for memory in all_memories:
            if memory.archived:
                continue

            # ì¤‘ìš”ë„ ê³„ì‚°
            factors = ImportanceFactors(
                access_count=memory.access_count,
                last_accessed=memory.last_accessed,
                created_at=memory.created_at,
                success_indicator=self._get_success_indicator(memory),
            )

            importance = self.calculator.calculate(factors)
            memory.importance_score = importance.total

            # ì•„ì¹´ì´ë¹™ ì¡°ê±´ ì²´í¬
            age_days = (datetime.utcnow() - memory.created_at).days
            if importance.total < self.importance_threshold and age_days > self.age_threshold:
                memory.archived = True
                self.repository.update(memory)
                archived_count += 1

        return archived_count

    def _get_success_indicator(self, memory: MemoryRecord) -> float:
        """outcome ê¸°ë°˜ ì„±ê³µ ì§€í‘œ"""
        if "ì„±ê³µ" in memory.outcome or "ì™„ë£Œ" in memory.outcome:
            return 1.0
        elif "ì‹¤íŒ¨" in memory.outcome or "ì˜¤ë¥˜" in memory.outcome:
            return 0.0
        else:
            return 0.5
```

### 4.4 Phase 4: ë©”ëª¨ë¦¬ í†µí•© ë° ì••ì¶•

#### 4.4.1 MemoryConsolidator

**íŒŒì¼**: `ai/memory/consolidator.py`

```python
class MemoryConsolidator:
    def __init__(self, repository, brain,
                 similarity_threshold=0.85, min_cluster_size=3):
        self.repository = repository
        self.brain = brain
        self.similarity_threshold = similarity_threshold
        self.min_cluster_size = min_cluster_size

    def consolidate(self):
        """ìœ ì‚¬í•œ ë©”ëª¨ë¦¬ í†µí•©"""
        memories = self.repository.list_all(archived=False)

        # 1. ì„ë² ë”© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        clusters = self._cluster_similar(memories)

        consolidated_count = 0
        for cluster in clusters:
            if len(cluster) < self.min_cluster_size:
                continue

            # 2. LLMìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ìš”ì•½
            summary = self._llm_summarize(cluster)

            # 3. í†µí•© ë©”ëª¨ë¦¬ ìƒì„±
            consolidated = MemoryRecord(
                summary=summary,
                goal=self._merge_goals(cluster),
                user_intent=self._merge_intents(cluster),
                outcome=self._merge_outcomes(cluster),
                category=MemoryCategory.WORKFLOW_OPTIMISATION,
                tools_used=self._unique_tools(cluster),
                tags=self._unique_tags(cluster),
                created_at=datetime.utcnow(),
                source_metadata={
                    "consolidated_from": [m.source_metadata["id"] for m in cluster],
                    "consolidation_date": datetime.utcnow().isoformat()
                }
            )

            # 4. ì €ì¥ ë° ì›ë³¸ ì•„ì¹´ì´ë¹™
            self.repository.add(consolidated)
            for memory in cluster:
                memory.archived = True
                memory.source_metadata["archived_by_consolidation"] = True
                self.repository.update(memory)

            consolidated_count += 1

        return consolidated_count

    def _cluster_similar(self, memories: List[MemoryRecord]) -> List[List[MemoryRecord]]:
        """ì„ë² ë”© ê¸°ë°˜ ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§"""
        from sklearn.cluster import AgglomerativeClustering
        import numpy as np

        # ì„ë² ë”© í–‰ë ¬ ìƒì„±
        embeddings = np.array([m.embedding for m in memories if m.embedding])

        # í´ëŸ¬ìŠ¤í„°ë§
        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=1 - self.similarity_threshold,
            linkage='average'
        )
        labels = clustering.fit_predict(embeddings)

        # í´ëŸ¬ìŠ¤í„°ë¡œ ê·¸ë£¹í™”
        clusters = {}
        for memory, label in zip(memories, labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(memory)

        return list(clusters.values())

    def _llm_summarize(self, cluster: List[MemoryRecord]) -> str:
        """LLMìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ìš”ì•½"""
        summaries = "\n".join([f"- {m.summary}" for m in cluster])

        prompt = f"""ë‹¤ìŒì€ ìœ ì‚¬í•œ ì‘ì—… ê²½í—˜ë“¤ì…ë‹ˆë‹¤. ì´ë¥¼ í•˜ë‚˜ì˜ í†µí•©ëœ ìš”ì•½ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

{summaries}

í†µí•© ìš”ì•½ (3-5 ë¬¸ì¥):"""

        response = self.brain.generate_text(prompt, temperature=0.3)
        return response.text.strip()
```

#### 4.4.2 ì •ê¸° ì‹¤í–‰ ìŠ¤ì¼€ì¤„ëŸ¬

```python
class MemoryMaintenanceScheduler:
    def __init__(self, memory_service,
                 consolidation_interval_hours=24,
                 archival_interval_hours=24):
        self.memory_service = memory_service
        self.consolidation_interval = consolidation_interval_hours
        self.archival_interval = archival_interval_hours
        self.last_consolidation = None
        self.last_archival = None

    def should_run_consolidation(self) -> bool:
        if self.last_consolidation is None:
            return True
        elapsed = datetime.utcnow() - self.last_consolidation
        return elapsed.total_seconds() / 3600 >= self.consolidation_interval

    def should_run_archival(self) -> bool:
        if self.last_archival is None:
            return True
        elapsed = datetime.utcnow() - self.last_archival
        return elapsed.total_seconds() / 3600 >= self.archival_interval

    def run_maintenance(self):
        """ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ìœ ì§€ë³´ìˆ˜"""
        results = {}

        if self.should_run_consolidation():
            consolidator = MemoryConsolidator(
                self.memory_service.repository,
                self.memory_service.brain
            )
            results["consolidated"] = consolidator.consolidate()
            self.last_consolidation = datetime.utcnow()

        if self.should_run_archival():
            archiver = MemoryArchiver(
                self.memory_service.repository,
                ImportanceCalculator()
            )
            results["archived"] = archiver.run_archival_pass()
            self.last_archival = datetime.utcnow()

        return results
```

### 4.5 Phase 5: CrewAI í†µí•©

#### 4.5.1 í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤

**íŒŒì¼**: `ai/memory/hybrid_service.py`

```python
class HybridMemoryService:
    """CrewAI Memory + Angmini Memory í†µí•©"""

    def __init__(self, crewai_crew, angmini_memory_service):
        self.crewai_crew = crewai_crew
        self.angmini = angmini_memory_service
        self.entity_repository = EntityRepository(...)

    def retrieve(self, query: str, context: str = "all", top_k: int = 10):
        """í†µí•© ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤"""
        results = []

        # 1. CrewAI Short-Term Memory
        if context in ["all", "short"]:
            # CrewAIì˜ short_term_memory ì ‘ê·¼
            stm_results = self.crewai_crew.memory.short_term.search(query, limit=5)
            results.extend([{"source": "short", "data": r} for r in stm_results])

        # 2. Entity Memory
        if context in ["all", "entity"]:
            entity_results = self.entity_repository.search_semantic(query, top_k=5)
            results.extend([{"source": "entity", "data": e, "score": s}
                           for e, s in entity_results])

        # 3. Angmini Long-Term Memory (Cascaded)
        if context in ["all", "long"]:
            ltm_result = self.angmini.repository.cascaded_retriever.retrieve(query)
            results.extend([{"source": "long", "data": m.record, "score": m.score}
                           for m in ltm_result.matches])

        # 4. ê²°ê³¼ ë³‘í•© ë° ë¦¬ë­í‚¹
        return self._merge_and_rank(results, top_k=top_k)

    def capture_from_task(self, task_result, user_request: str):
        """Task ì™„ë£Œ ì‹œ ìë™ìœ¼ë¡œ Angmini LTMì— ì €ì¥"""
        # ExecutionContext ìƒì„±
        context = self._build_context_from_task(task_result)

        # Angmini ë©”ëª¨ë¦¬ ìº¡ì²˜
        capture_result = self.angmini.capture(context, user_request)

        # ì—”í‹°í‹° ì¶”ì¶œ ë° ì €ì¥
        entities = self._extract_entities(context)
        for entity in entities:
            self.entity_repository.add_or_update(entity)

        return capture_result

    def _merge_and_rank(self, results, top_k):
        """ì†ŒìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ ë¦¬ë­í‚¹"""
        weights = {
            "short": 0.4,   # ìµœê·¼ ì»¨í…ìŠ¤íŠ¸ ì¤‘ìš”
            "entity": 0.3,  # ê´€ê³„ ì •ë³´ ì¤‘ìš”
            "long": 0.3     # ê³¼ê±° ê²½í—˜
        }

        scored = []
        for item in results:
            source = item["source"]
            base_score = item.get("score", 1.0)
            final_score = base_score * weights[source]
            scored.append((item["data"], final_score))

        scored.sort(key=lambda x: x[1], reverse=True)
        return [data for data, score in scored[:top_k]]
```

#### 4.5.2 Crew ì„¤ì • ì—…ë°ì´íŠ¸

**íŒŒì¼**: `crew/crew_config.py`

```python
class AngminiCrew:
    def __init__(self, ai_brain, memory_service, verbose=True):
        self.ai_brain = ai_brain
        self.angmini_memory = memory_service

        # CrewAI ì—ì´ì „íŠ¸ ìƒì„±
        self.planner = AgentFactory.create_planner(ai_brain, memory_service)
        self.worker_agents = AgentFactory.create_all_agents(ai_brain, memory_service)

        # CrewAI Crew ìƒì„± (built-in memory í™œì„±í™”)
        self.crew = Crew(
            agents=[agent.build_agent() for agent in self.worker_agents],
            tasks=[],
            process=Process.hierarchical,
            manager_agent=self.planner.build_agent(),
            memory=True,  # CrewAI Short-Term + Long-Term + Entity
            embedder={
                "provider": "huggingface",
                "config": {"model": "Qwen/Qwen3-Embedding-0.6B"}
            },
            verbose=verbose
        )

        # í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ìƒì„±
        self.hybrid_memory = HybridMemoryService(
            crewai_crew=self.crew,
            angmini_memory_service=self.angmini_memory
        )

    def kickoff(self, user_input: str) -> str:
        """Task ì‹¤í–‰ ë° ë©”ëª¨ë¦¬ ìº¡ì²˜"""
        # Task ìƒì„± ë° ì‹¤í–‰
        tasks = TaskFactory.create_tasks_from_input(user_input)
        result = self.crew.kickoff(tasks)

        # Angmini Long-Term Memoryì— ì €ì¥
        if self._should_capture_memory(result):
            self.hybrid_memory.capture_from_task(result, user_input)

        return str(result)
```

---

## 5. êµ¬í˜„ ë¡œë“œë§µ

### 5.1 ìš°ì„ ìˆœìœ„ (P0 â†’ P1 â†’ P2)

#### P0: ì¦‰ì‹œ êµ¬í˜„ (Critical Path)

**1. Entity Memory ì‹œìŠ¤í…œ** â±ï¸ 3-5ì¼
- **ë³µì¡ë„**: ì¤‘ê°„
- **ì˜í–¥**: ë†’ìŒ
- **íŒŒì¼**:
  - `ai/memory/entity/models.py`
  - `ai/memory/entity/extractor.py`
  - `ai/memory/entity/storage.py`
  - `ai/memory/entity/tracker.py`
- **ì˜ì¡´ì„±**: ì—†ìŒ
- **í…ŒìŠ¤íŠ¸**:
  - `tests/memory/entity/test_extractor.py`
  - `tests/memory/entity/test_storage.py`

**2. Hybrid Search (ì˜ë¯¸ì  + í‚¤ì›Œë“œ)** â±ï¸ 2-3ì¼
- **ë³µì¡ë„**: ë‚®ìŒ
- **ì˜í–¥**: ë†’ìŒ
- **íŒŒì¼**:
  - `ai/memory/storage/hybrid_retriever.py`
  - SQLite ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ (FTS5 í…Œì´ë¸” ìƒì„±)
- **ì˜ì¡´ì„±**: ì—†ìŒ
- **í…ŒìŠ¤íŠ¸**:
  - `tests/memory/test_hybrid_retriever.py`

#### P1: ë‹¤ìŒ ìŠ¤í”„ë¦°íŠ¸ (Important)

**3. Short-Term Memory (CrewAI í†µí•©)** â±ï¸ 1-2ì¼
- **ë³µì¡ë„**: ë‚®ìŒ (CrewAIê°€ ì œê³µ)
- **ì˜í–¥**: ì¤‘ê°„
- **íŒŒì¼**:
  - `crew/crew_config.py` ìˆ˜ì • (`memory=True`)
- **ì˜ì¡´ì„±**: CrewAI ì—…ê·¸ë ˆì´ë“œ í•„ìš”ì‹œ
- **í…ŒìŠ¤íŠ¸**:
  - `tests/crew/test_crewai_memory.py`

**4. Importance Score ì‹œìŠ¤í…œ** â±ï¸ 2-3ì¼
- **ë³µì¡ë„**: ì¤‘ê°„
- **ì˜í–¥**: ì¤‘ê°„
- **íŒŒì¼**:
  - `ai/memory/importance.py`
  - `ai/memory/archiver.py`
  - `ai/memory/memory_records.py` (ìŠ¤í‚¤ë§ˆ í™•ì¥)
- **ì˜ì¡´ì„±**: None
- **í…ŒìŠ¤íŠ¸**:
  - `tests/memory/test_importance.py`

**5. Hybrid Memory Service (í†µí•© ë ˆì´ì–´)** â±ï¸ 2-3ì¼
- **ë³µì¡ë„**: ì¤‘ê°„
- **ì˜í–¥**: ë†’ìŒ
- **íŒŒì¼**:
  - `ai/memory/hybrid_service.py`
  - `crew/crew_config.py` (í†µí•©)
- **ì˜ì¡´ì„±**: P0 (Entity), P1 (STM)
- **í…ŒìŠ¤íŠ¸**:
  - `tests/memory/test_hybrid_service.py`

#### P2: ë¯¸ë˜ ê°œì„  (Nice to Have)

**6. Memory Consolidation** â±ï¸ 3-4ì¼
- **ë³µì¡ë„**: ë†’ìŒ
- **ì˜í–¥**: ë‚®ìŒ (ì´ˆê¸° ë©”ëª¨ë¦¬ ìˆ˜ ì ìŒ)
- **íŒŒì¼**:
  - `ai/memory/consolidator.py`
  - `ai/memory/scheduler.py`
- **ì˜ì¡´ì„±**: P1 (Importance)
- **íŠ¸ë¦¬ê±°**: ë©”ëª¨ë¦¬ ìˆ˜ > 1000ê°œ

**7. ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ í™•ì¥** â±ï¸ 2-3ì¼
- **ë³µì¡ë„**: ì¤‘ê°„
- **ì˜í–¥**: ë‚®ìŒ (ë””ë²„ê¹…ìš©)
- **íŒŒì¼**:
  - `ai/memory/events.py`
  - `ai/memory/metrics.py` (í™•ì¥)
- **ì˜ì¡´ì„±**: None

### 5.2 ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

#### 5.2.1 ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜

**ìŠ¤í¬ë¦½íŠ¸**: `scripts/migrate_memory_v2.py`

```python
"""ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ v2 ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸"""
import sqlite3
from pathlib import Path

def migrate_to_v2(db_path: Path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # 1. MemoryRecordì— ìƒˆ í•„ë“œ ì¶”ê°€
    new_columns = [
        ("access_count", "INTEGER DEFAULT 0"),
        ("last_accessed", "TEXT"),
        ("importance_score", "REAL"),
        ("archived", "INTEGER DEFAULT 0"),
    ]

    for col_name, col_type in new_columns:
        try:
            cursor.execute(f"ALTER TABLE memories ADD COLUMN {col_name} {col_type}")
        except sqlite3.OperationalError:
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼
            pass

    # 2. FTS5 í…Œì´ë¸” ìƒì„±
    cursor.execute("""
        CREATE VIRTUAL TABLE IF NOT EXISTS memories_fts
        USING fts5(
            id UNINDEXED,
            summary,
            goal,
            user_intent,
            outcome,
            tags
        )
    """)

    # 3. ê¸°ì¡´ ë°ì´í„°ë¥¼ FTS5ì— ì¸ë±ì‹±
    cursor.execute("""
        INSERT INTO memories_fts(id, summary, goal, user_intent, outcome, tags)
        SELECT id, summary, goal, user_intent, outcome, tags
        FROM memories
    """)

    # 4. Entity í…Œì´ë¸” ìƒì„±
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS entities (
            id TEXT PRIMARY KEY,
            type TEXT NOT NULL,
            name TEXT NOT NULL,
            attributes TEXT,
            first_seen TEXT NOT NULL,
            last_seen TEXT NOT NULL,
            mention_count INTEGER DEFAULT 1,
            relations TEXT,
            embedding BLOB
        )
    """)

    # 5. Entity ì¸ë±ìŠ¤
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_entities_type_name
        ON entities(type, name)
    """)

    conn.commit()
    conn.close()

    print("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")

if __name__ == "__main__":
    from ai.core.config import Config
    config = Config.load()

    db_path = Path("data/memory/memories.db")
    migrate_to_v2(db_path)
```

#### 5.2.2 ê¸°ì¡´ ë©”ëª¨ë¦¬ ì—”í‹°í‹° ì¶”ì¶œ

**ìŠ¤í¬ë¦½íŠ¸**: `scripts/extract_entities_from_existing.py`

```python
"""ê¸°ì¡´ ë©”ëª¨ë¦¬ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ"""
from ai.memory.factory import create_memory_service
from ai.memory.entity.extractor import EntityExtractor
from ai.memory.entity.storage import EntityRepository
from ai.ai_brain import AIBrain

def extract_entities_from_existing():
    brain = AIBrain()
    memory_service = create_memory_service()
    extractor = EntityExtractor(brain)
    entity_repo = EntityRepository(...)

    all_memories = memory_service.repository.list_all()

    for i, memory in enumerate(all_memories):
        print(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(all_memories)}")

        # í…ìŠ¤íŠ¸ êµ¬ì„±
        text = f"{memory.summary}\n{memory.goal}\n{memory.outcome}"

        # ì—”í‹°í‹° ì¶”ì¶œ
        entities = extractor.extract(text)

        # ì €ì¥
        for entity in entities:
            entity_repo.add_or_update(entity)

    print(f"âœ… {len(all_memories)}ê°œ ë©”ëª¨ë¦¬ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ ì™„ë£Œ")

if __name__ == "__main__":
    extract_entities_from_existing()
```

#### 5.2.3 Feature Flags

**í™˜ê²½ë³€ìˆ˜**: `.env`

```bash
# ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ v2 ê¸°ëŠ¥ í”Œë˜ê·¸
MEMORY_V2_ENTITY_ENABLED=true
MEMORY_V2_HYBRID_SEARCH_ENABLED=true
MEMORY_V2_IMPORTANCE_ENABLED=false  # ì ì§„ì  í™œì„±í™”
MEMORY_V2_CONSOLIDATION_ENABLED=false  # ë¯¸ë˜ì— í™œì„±í™”
```

**ì½”ë“œ**:
```python
from ai.core.config import Config

config = Config.load()

if config.get("MEMORY_V2_ENTITY_ENABLED", "false") == "true":
    # Entity Memory í™œì„±í™”
    entity_repository = EntityRepository(...)
else:
    entity_repository = None
```

### 5.3 í…ŒìŠ¤íŠ¸ ì „ëµ

#### 5.3.1 Unit Tests

**Entity Extractor**:
```python
# tests/memory/entity/test_extractor.py
def test_entity_extraction():
    brain = AIBrain()
    extractor = EntityExtractor(brain)

    text = "ê¹€ì² ìˆ˜ê°€ Django í”„ë¡œì íŠ¸ì—ì„œ pytestë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
    entities = extractor.extract(text)

    assert len(entities) >= 3
    assert any(e.type == EntityType.PERSON and "ê¹€ì² ìˆ˜" in e.name for e in entities)
    assert any(e.type == EntityType.PROJECT and "Django" in e.name for e in entities)
    assert any(e.type == EntityType.TOOL and "pytest" in e.name for e in entities)
```

**Hybrid Retriever**:
```python
# tests/memory/test_hybrid_retriever.py
def test_rrf_fusion():
    retriever = HybridRetriever(...)

    # "Django í”„ë¡œì íŠ¸" ê²€ìƒ‰
    results = retriever.search("Django í”„ë¡œì íŠ¸", top_k=5)

    assert len(results) <= 5
    assert all(isinstance(r, tuple) and len(r) == 2 for r in results)

    # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ í™•ì¸
    scores = [score for _, score in results]
    assert scores == sorted(scores, reverse=True)
```

**Importance Calculator**:
```python
# tests/memory/test_importance.py
def test_importance_calculation():
    calculator = ImportanceCalculator()

    # ë†’ì€ ì¤‘ìš”ë„: ìì£¼ ì ‘ê·¼, ìµœê·¼, ì„±ê³µ
    factors_high = ImportanceFactors(
        access_count=50,
        created_at=datetime.utcnow() - timedelta(days=1),
        success_indicator=1.0
    )
    score_high = calculator.calculate(factors_high)

    # ë‚®ì€ ì¤‘ìš”ë„: ì ‘ê·¼ ì—†ìŒ, ì˜¤ë˜ë¨, ì‹¤íŒ¨
    factors_low = ImportanceFactors(
        access_count=0,
        created_at=datetime.utcnow() - timedelta(days=180),
        success_indicator=0.0
    )
    score_low = calculator.calculate(factors_low)

    assert score_high.total > score_low.total
    assert score_high.total > 0.7
    assert score_low.total < 0.3
```

#### 5.3.2 Integration Tests

**ì „ì²´ íŒŒì´í”„ë¼ì¸**:
```python
# tests/memory/test_integration.py
def test_full_memory_pipeline():
    brain = AIBrain()
    memory_service = MemoryService.build(brain)

    # ë©”ëª¨ë¦¬ ìº¡ì²˜
    context = create_test_context()
    result = memory_service.capture(context, "í…ŒìŠ¤íŠ¸ ìš”ì²­")

    assert result.stored
    assert result.record_id is not None

    # ê²€ìƒ‰ (í•˜ì´ë¸Œë¦¬ë“œ)
    search_results = memory_service.repository.hybrid_search("í…ŒìŠ¤íŠ¸", top_k=5)
    assert len(search_results) > 0

    # ì—”í‹°í‹° í™•ì¸
    entities = entity_repository.find_by_name("í…ŒìŠ¤íŠ¸")
    assert len(entities) > 0
```

#### 5.3.3 Performance Tests

**ê²€ìƒ‰ ì„±ëŠ¥**:
```python
# tests/memory/test_performance.py
import time

def test_search_latency():
    retriever = HybridRetriever(...)

    # 100íšŒ ê²€ìƒ‰
    latencies = []
    for _ in range(100):
        start = time.perf_counter()
        retriever.search("test query", top_k=10)
        latency = (time.perf_counter() - start) * 1000  # ms
        latencies.append(latency)

    # P95 < 100ms
    p95 = sorted(latencies)[95]
    assert p95 < 100.0, f"P95 latency {p95:.2f}ms > 100ms"
```

**ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**:
```python
import psutil
import os

def test_memory_usage_10k_records():
    process = psutil.Process(os.getpid())

    # 10,000ê°œ ë©”ëª¨ë¦¬ ë¡œë“œ
    repository = MemoryRepository(...)
    memories = repository.list_all()[:10000]

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    mem_mb = process.memory_info().rss / 1024 / 1024

    # < 500MB
    assert mem_mb < 500, f"Memory usage {mem_mb:.2f}MB > 500MB"
```

#### 5.3.4 Quality Tests

**ê²€ìƒ‰ ì •í™•ë„ (Precision@K)**:
```python
def test_search_precision():
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (ì¿¼ë¦¬ â†’ ê´€ë ¨ ë©”ëª¨ë¦¬ ID)
    test_queries = {
        "Django í”„ë¡œì íŠ¸ ì‘ì—…": ["mem_001", "mem_045", "mem_103"],
        "pytest ì˜¤ë¥˜ í•´ê²°": ["mem_012", "mem_089"],
    }

    retriever = HybridRetriever(...)
    precisions = []

    for query, relevant_ids in test_queries.items():
        results = retriever.search(query, top_k=5)
        result_ids = [r.source_metadata["id"] for r, _ in results]

        # Precision@5
        relevant_in_top5 = len(set(result_ids) & set(relevant_ids))
        precision = relevant_in_top5 / 5
        precisions.append(precision)

    avg_precision = sum(precisions) / len(precisions)
    assert avg_precision > 0.8, f"Precision@5 {avg_precision:.2f} < 0.8"
```

---

## 6. ì˜ˆìƒ íš¨ê³¼

### 6.1 ì •ëŸ‰ì  ê°œì„ 

| ë©”íŠ¸ë¦­ | í˜„ì¬ | ëª©í‘œ | ê°œì„ ìœ¨ |
|--------|------|------|--------|
| **ê²€ìƒ‰ ì •í™•ë„** (Precision@5) | 0.65 | 0.85 | +30% |
| **ì»¨í…ìŠ¤íŠ¸ ì¸ì‹** (ì—”í‹°í‹° ì¶”ì ) | 0% | 80% | +80% |
| **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±** (ì•„ì¹´ì´ë¹™) | N/A | 40% | +40% |
| **ê²€ìƒ‰ ì†ë„** (P95 latency) | 150ms | <100ms | +33% |
| **ì¤‘ë³µ ê°ì†Œ** | 5% | <2% | +60% |

### 6.2 ì •ì„±ì  ê°œì„ 

âœ… **ì‚¬ìš©ì ê²½í—˜**:
- ë” ì •í™•í•œ ê³¼ê±° ê²½í—˜ ê²€ìƒ‰
- "ì´ ì‚¬ìš©ìê°€ ìì£¼ ì“°ëŠ” ë„êµ¬ëŠ”?" ê°™ì€ ì§ˆë¬¸ ê°€ëŠ¥
- ê°œì¸í™”ëœ ì‘ë‹µ ìƒì„±

âœ… **ì‹œìŠ¤í…œ ì„±ëŠ¥**:
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
- ì•„ì¹´ì´ë¹™ìœ¼ë¡œ ì¥ê¸° ì„±ëŠ¥ ìœ ì§€
- ë©”ëª¨ë¦¬ í†µí•©ìœ¼ë¡œ ìŠ¤í† ë¦¬ì§€ ìµœì í™”

âœ… **ìœ ì§€ë³´ìˆ˜ì„±**:
- CrewAI ë„¤ì´í‹°ë¸Œ ë©”ëª¨ë¦¬ì™€ í†µí•©
- ëª…í™•í•œ ê³„ì¸µ ë¶„ë¦¬ (STM/LTM/Entity)
- ì´ë²¤íŠ¸ ì‹œìŠ¤í…œìœ¼ë¡œ ë””ë²„ê¹… ìš©ì´

### 6.3 CrewAI ì‹œë„ˆì§€

**Before** (ë‹¨ì¼ Long-Term Memory):
```
ì‚¬ìš©ì ìš”ì²­
    â†“
Cascaded Retrieval (ê³¼ê±° ê²½í—˜ë§Œ)
    â†“
ì‘ë‹µ ìƒì„±
```

**After** (ê³„ì¸µì  í†µí•© Memory):
```
ì‚¬ìš©ì ìš”ì²­
    â†“
Hybrid Memory Service
    â”œâ”€ Short-Term (ìµœê·¼ ëŒ€í™”)
    â”œâ”€ Entity (ê´€ê³„ ì •ë³´)
    â””â”€ Long-Term (ê³¼ê±° ê²½í—˜)
    â†“
í†µí•© ë¦¬ë­í‚¹
    â†“
ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì‘ë‹µ
```

**íš¨ê³¼**:
- **ë‹¨ê¸° ì»¨í…ìŠ¤íŠ¸**: "ë°©ê¸ˆ ë§í•œ í”„ë¡œì íŠ¸" ì´í•´
- **ê´€ê³„ ì¸ì‹**: "ê¹€ì² ìˆ˜ê°€ ì‘ì—…í•œ í”„ë¡œì íŠ¸ë“¤"
- **ì¥ê¸° í•™ìŠµ**: "ê³¼ê±° ìœ ì‚¬ ì‘ì—…ì—ì„œ ì„±ê³µí•œ ë°©ë²•"

---

## 7. ì°¸ê³  ìë£Œ

### 7.1 ë…¼ë¬¸ ë° ë¬¸ì„œ

1. **MemGPT / Letta**:
   - https://www.letta.com/blog/benchmarking-ai-agent-memory
   - LoCoMo Benchmark ê²°ê³¼

2. **Mem0 Platform**:
   - arXiv:2504.19413 "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory"

3. **AutoGen v0.4**:
   - https://www.microsoft.com/en-us/research/blog/autogen-v0-4-reimagining-the-foundation-of-agentic-ai-for-scale-extensibility-and-robustness/

4. **CrewAI Memory**:
   - https://docs.crewai.com/concepts/memory

5. **LangChain Memory**:
   - https://python.langchain.com/docs/how_to/migrate_agent
   - https://python.langchain.com/docs/tutorials/qa_chat_history

### 7.2 ì½”ë“œ ì°¸ê³ 

- **Reciprocal Rank Fusion**: https://github.com/Raudaschl/rag-fusion
- **Entity Extraction with LLM**: LangChain NER examples
- **Temporal Decay**: Information Retrieval í‘œì¤€ ê³µì‹

---

## 8. ë‹¤ìŒ ë‹¨ê³„

### 8.1 ì¦‰ì‹œ ì‹¤í–‰ (ì´ë²ˆ ì£¼)

1. âœ… **ì„¤ê³„ ë¬¸ì„œ ì‘ì„± ì™„ë£Œ** (í˜„ì¬ ë¬¸ì„œ)
2. ğŸ”„ **ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**
3. ğŸ”„ **Entity Memory ì‹œìŠ¤í…œ êµ¬í˜„ ì‹œì‘**

### 8.2 ë‹¤ìŒ ì£¼

1. Entity Memory ì™„ë£Œ ë° í…ŒìŠ¤íŠ¸
2. Hybrid Search êµ¬í˜„
3. í†µí•© í…ŒìŠ¤íŠ¸

### 8.3 í–¥í›„ ê³„íš

1. Importance Score ì‹œìŠ¤í…œ (2ì£¼ì°¨)
2. Hybrid Memory Service í†µí•© (3ì£¼ì°¨)
3. Memory Consolidation (4ì£¼ì°¨)

---

**ì‘ì„±**: Claude Code (Anthropic)
**ê²€í†  í•„ìš”**: ê°œë°œ íŒ€
**ìƒíƒœ**: âœ… ì„¤ê³„ ì™„ë£Œ, êµ¬í˜„ ì¤€ë¹„ ì™„ë£Œ
